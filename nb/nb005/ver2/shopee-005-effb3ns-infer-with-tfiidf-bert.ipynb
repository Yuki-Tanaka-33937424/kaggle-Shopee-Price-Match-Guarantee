{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Directory settiings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR='./'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \nROOT_DIR = '../input/shopee-product-matching/'\nTRAIN_PATH = ROOT_DIR + 'train_images/'\nTEST_PATH = ROOT_DIR + 'test_images/'","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## CFG","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug = True\n    CHECK_SUB = False\n    GET_CV = True\n    num_workers = 4\n    model_name_cnn = 'tf_efficientnet_b3_ns'\n    model_name_bert = '../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n    size = 512\n    batch_size = 8\n    seed = 42\n    target_size = 8811\n    target_size_list = [8811, 8812, 8811, 8811, 8811]\n    target_col = 'label_group'\n    use_fc = False\n    use_arcface = True\n    scale = 30\n    margin = 0.5\n    fc_dim = 512\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n    train = False\n    inference = True","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntest = pd.read_csv('../input/shopee-product-matching/test.csv')\nif len(test)>3: \n    CFG.GET_CV = False\nelse: \n    print('this submission notebook will compute CV score, but commit notebook will not')","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"this submission notebook will compute CV score, but commit notebook will not\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Library","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, _LRScheduler\n\nimport transformers\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\n\nimport timm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection / (len_y_pred + len_y_true)\n    return f1\n\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\ndef init_logger(log_file=OUTPUT_DIR+'inference.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n#LOGGER = init_logger()\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(CFG.model_name_bert)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"def read_dataset():\n    if CFG.GET_CV:\n        \n        # create folds\n        # trainingの時と同じようにfoldを切っています。\n        folds = pd.read_csv('../input/shopee-product-matching/train.csv')\n        if CFG.debug:\n            folds = folds.sample(n=200, random_state=CFG.seed).reset_index(drop=True)  \n        Fold = GroupKFold(n_splits=CFG.n_fold)\n        groups = folds['label_group'].values\n        for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col], groups)):\n            folds.loc[val_index, 'fold'] = int(n)\n        folds['fold'] = folds['fold'].astype(int)\n        display(folds.groupby('fold').size())\n        \n        tmp = folds.groupby('label_group')['posting_id'].unique().to_dict()\n        folds['matches'] = folds['label_group'].map(tmp)\n        folds['matches'] = folds['matches'].apply(lambda x: ' '.join(x))\n        folds['file_path'] = folds['image'].apply(lambda x: TRAIN_PATH + x)\n        \n        if CFG.CHECK_SUB:\n            folds = pd.concat([folds, folds], axis=0)\n            folds.reset_index(drop=True, inplace=True)\n        folds_cu = cudf.DataFrame(folds)\n    else:\n        folds = pd.read_csv('../input/shopee-product-matching/test.csv')\n        folds['file_path'] = folds['image'].apply(lambda x: TEST_PATH + x)\n        folds_cu = cudf.DataFrame(folds)\n        \n    return folds, folds_cu","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n            \n        return image, torch.tensor(1)","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TestDataset_BERT(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        text = self.df.loc[idx, 'title']\n        text = tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')  # 'pt': pytorch\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]\n        return input_ids, attention_mask","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            #Resize(CFG.size, CFG.size),\n            RandomResizedCrop(CFG.size, CFG.size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n        \n    def forward(self, input, label):\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n    \n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output, nn.CrossEntropyLoss()(output,label)\n\nclass CustomEfficientNet(nn.Module):\n    \n    def __init__(\n        self,\n        n_classes = CFG.target_size,\n        model_name = CFG.model_name_cnn,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = True):\n        \n        super(CustomEfficientNet,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        self.use_fc = use_fc\n        \n        if use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n        \n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n    def forward(self, image, label):\n        features = self.extract_features(image)\n        if self.training:\n            logits = self.final(features, label)\n            return logits\n        else:\n            return features\n        \n    def extract_features(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc and self.training:\n            x = self.dropout(x)\n            x = self.classifier(x)\n            x = self.bn(x)\n        return x","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class CustomBERT(nn.Module):\n    def __init__(\n        self,\n        n_classes = CFG.target_size,\n        model_name = CFG.model_name_bert,\n        fc_dim = CFG.fc_dim,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = CFG.use_fc,\n        use_arcface = CFG.use_arcface,\n        pretrained = True):\n        \n        super(CustomBERT, self).__init__()\n        print(f'Building Model Backbone for {model_name} model')\n        self.bert = transformers.AutoModel.from_pretrained(model_name)\n        in_features = self.bert.config.hidden_size\n        self.use_fc = use_fc\n        self.use_arcface = use_arcface\n        \n        if self.use_fc:\n            self.dropout = nn.Dropout(p=0.1)\n            self.classifier = nn.Linear(in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            in_features = fc_dim\n        \n        if self.use_arcface:\n            self.final = ArcMarginProduct(\n            in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n        else:\n            self.final = nn.Linear(in_features, n_classes)\n    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.classifier.weight)\n        nn.init.constant_(self.classifier.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n        \n    def forward(self, input_ids, attention_mask):\n        features = self.extract_features(input_ids, attention_mask)\n        return features\n        \n    def extract_features(self, input_ids, attention_mask):\n        x = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        features = x[0]\n        features = features[:, 0, :]\n        \n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.classifier(features)\n            features = self.bn(features)\n        return features","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## inference functions","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(folds, fold):\n    \n    CFG.target_size = CFG.target_size_list[fold]\n    model = CustomEfficientNet(n_classes=CFG.target_size, pretrained=False).to(device)\n    model_path = f'../input/shopee-002-data-local/tf_efficientnet_b3_ns_fold{fold}_best.pth'\n    model.load_state_dict(torch.load(model_path)['model'])\n    model.eval()\n    \n    image_dataset = TestDataset(folds, transform=get_transforms(data='valid'))\n    image_loader = DataLoader(image_dataset,\n                              batch_size=CFG.batch_size,\n                              num_workers=CFG.num_workers,\n                              pin_memory=True,\n                              drop_last=False)\n    embeds = []\n    with torch.no_grad():\n        pbar = tqdm(image_loader, total=len(image_loader))\n        for img, label in pbar:\n            img = img.to(device)\n            label = label.to(device)\n            features = model(img, label)\n            image_embeddings = features.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n            \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_text_embeddings(folds, fold):\n    \n    CFG.target_size = CFG.target_size_list[fold]\n    model = CustomBERT(n_classes=CFG.target_size, pretrained=False).to(device)\n    model_path = f'../input/hopee-004-bert-training-data/paraphrase-xlm-r-multilingual-v1_fold{fold}_best.pth'\n    model.load_state_dict(torch.load(model_path)['model'])\n    model.eval()\n    \n    text_dataset = TestDataset_BERT(folds)\n    text_loader = DataLoader(text_dataset,\n                              batch_size=CFG.batch_size,\n                              num_workers=CFG.num_workers,\n                              pin_memory=True,\n                              drop_last=False)\n    embeds = []\n    with torch.no_grad():\n        pbar = tqdm(text_loader, total=len(text_loader))\n        for input_ids, attention_mask in pbar:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            features = model(input_ids, attention_mask)\n            text_embeddings = features.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n            \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_text_predictions(df, df_cu, max_features=25_000):\n    \n    model = TfidfVectorizer(stop_words='english',\n                            binary=True,\n                            max_features=max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    \n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) // CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n        \n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        print('chunk', a, 'to', b)\n        \n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n        \n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>0.75)[0]  # 変える余地がありそう\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    del model, text_embeddings\n    gc.collect()\n    return preds","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_neighbors(df, embeddings, KNN = 50, image = True):\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric='cosine')\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if CFG.GET_CV:\n        if image:\n            thresholds = list(np.arange(0.4, 0.5, 0.1))\n        else:\n            thresholds = list(np.arange(0.4, 0.6, 0.1))  # changed\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k, idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold  = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < 0.3)[0]\n            else:\n                idx = np.where(distances[k,] < 0.3)[0]\n            ids = indices[k, idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n            \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < 0.3)[0]\n            else:\n                idx = np.where(distances[k,] < 0.3)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Predictions","metadata":{}},{"cell_type":"code","source":"folds, folds_cu = read_dataset()\nfolds.head()","metadata":{"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold\n0    40\n1    40\n2    40\n3    40\n4    40\ndtype: int64"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0  train_3994562790  8c79f5e57737e712428b009f3b811b29.jpg  f5c972349712386d   \n1  train_2721383565  5dc0a645807ec85ba540e39ee7a1db04.jpg  b939c692c34e9867   \n2   train_161578771  e4647b784f847e807173e9329c748636.jpg  bbb84c822c86e767   \n3   train_481272730  9bf9a36b8a4f81ee1f4a0b463799e5b3.jpg  8af3f7809c5b0ce1   \n4  train_2554398684  5c138e8fccfc7bc6d6e364bd31eb6657.jpg  9b7ae0c595b38307   \n\n                                               title  label_group  fold  \\\n0  Creamer Nabati (Non Dairy Creamer) Premium & R...   3829989058     3   \n1   Chek Hup 3 in 1 Ipoh White Coffee King 40gr x 15   2723899357     4   \n2  ( 27AN.ID) COD Kacamata Hitam KM50 Gaya Steamp...   2217561812     1   \n3          Sapu Mini + Cikrak Sebaguna (Paket Hemat)    424835120     4   \n4  SARUNG TANGAN IGLOVE TOUCH SCREEN HP ANDROID G...     66247839     1   \n\n                             matches  \\\n0                   train_3994562790   \n1                   train_2721383565   \n2                    train_161578771   \n3                    train_481272730   \n4  train_2554398684 train_4236194881   \n\n                                           file_path  \n0  ../input/shopee-product-matching/train_images/...  \n1  ../input/shopee-product-matching/train_images/...  \n2  ../input/shopee-product-matching/train_images/...  \n3  ../input/shopee-product-matching/train_images/...  \n4  ../input/shopee-product-matching/train_images/...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>fold</th>\n      <th>matches</th>\n      <th>file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_3994562790</td>\n      <td>8c79f5e57737e712428b009f3b811b29.jpg</td>\n      <td>f5c972349712386d</td>\n      <td>Creamer Nabati (Non Dairy Creamer) Premium &amp; R...</td>\n      <td>3829989058</td>\n      <td>3</td>\n      <td>train_3994562790</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_2721383565</td>\n      <td>5dc0a645807ec85ba540e39ee7a1db04.jpg</td>\n      <td>b939c692c34e9867</td>\n      <td>Chek Hup 3 in 1 Ipoh White Coffee King 40gr x 15</td>\n      <td>2723899357</td>\n      <td>4</td>\n      <td>train_2721383565</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_161578771</td>\n      <td>e4647b784f847e807173e9329c748636.jpg</td>\n      <td>bbb84c822c86e767</td>\n      <td>( 27AN.ID) COD Kacamata Hitam KM50 Gaya Steamp...</td>\n      <td>2217561812</td>\n      <td>1</td>\n      <td>train_161578771</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_481272730</td>\n      <td>9bf9a36b8a4f81ee1f4a0b463799e5b3.jpg</td>\n      <td>8af3f7809c5b0ce1</td>\n      <td>Sapu Mini + Cikrak Sebaguna (Paket Hemat)</td>\n      <td>424835120</td>\n      <td>4</td>\n      <td>train_481272730</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_2554398684</td>\n      <td>5c138e8fccfc7bc6d6e364bd31eb6657.jpg</td>\n      <td>9b7ae0c595b38307</td>\n      <td>SARUNG TANGAN IGLOVE TOUCH SCREEN HP ANDROID G...</td>\n      <td>66247839</td>\n      <td>1</td>\n      <td>train_2554398684 train_4236194881</td>\n      <td>../input/shopee-product-matching/train_images/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Get neighbors for image_embeddings\nif CFG.GET_CV:\n#     oof_df = pd.DataFrame()\n    image_embeddings = []\n    text_embeddings_bert = []\n    for fold in CFG.trn_fold:\n        folds_ = folds[folds['fold'] == fold].reset_index(drop=True)\n        image_embeddings_ = get_image_embeddings(folds_, fold)\n        text_embeddings_bert_ = get_text_embeddings(folds_, fold)\n        image_embeddings.append(image_embeddings)\n        text_embeddings_bert.append(text_embeddings_bert_)\n        torch.cuda.empty_cache()\n    image_embeddings = np.concatenate(image_embeddings)\n    text_embeddings_bert = np.concatenate(text_embeddings_bert)\n    text_predictions_tfidf = get_text_predictions(folds, folds_cu, max_features=25_000)\n    oof_df, image_predictions = get_neighbors(folds, image_embeddings, KNN=50 if len(folds)>3 else 3, image=True)\n    oof_df, text_predictions_bert = get_neighbors(folds_, text_embeddings, KNN=50 if len(folds) > 3 else 3, image=False)\n    oof_df['image_predictions'] = image_predictions\n    oof_df['text_predictions'] = text_predictions_tfidf\n    oof_df['text_predictions_bert_tfidf'] = text_predictions_bert\n    oof_df['text_predictions_bert_len'] = oof_df_['text_predictions_bert'].apply(lambda x: len(x))\n    oof_df['text_predictions'] = oof_df['text_predictions_tfidf'].mask(oof_df_['text_predictions_bert_len'] == 2, oof_df_['text_predictions_bert'], inplace=True)\n    oof_df['pred_matches'] = oof_df_.apply(combine_predictions, axis = 1)\n    # oofだけを切り出す\n#         oof_df_ = oof_df_[folds['fold'] == fold]\n    display(oof_df.head())\nelse:\n    image_embeddings = get_image_embeddings(folds, fold=0)  # 後で調整する\n    text_embeddings = get_text_embeddings(folds, fold=0)\n    text_predictions_tfidf = get_text_predictions(folds, folds_cu, max_features=25_000) \n    df, text_predictions_bert = get_neighbors(folds, text_embeddings, KNN=50 if len(folds) > 3 else 3, image=False)\n    df, image_predictions = get_neighbors(folds, image_embeddings, KNN=50 if len(folds)>3 else 3, image=True)\n    \n    df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Building Model Backbone for tf_efficientnet_b3_ns model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d055a0e6f00b41aaadf25eacefbff04d"}},"metadata":{}},{"name":"stdout","text":"Our image embeddings shape is (40, 1536)\nBuilding Model Backbone for ../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017f6bfe836c42c5add36f4e1d584f84"}},"metadata":{}},{"name":"stdout","text":"Our text embeddings shape is (40, 768)\nBuilding Model Backbone for tf_efficientnet_b3_ns model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5351da0792e84742b922716289a6a747"}},"metadata":{}},{"name":"stdout","text":"Our image embeddings shape is (40, 1536)\nBuilding Model Backbone for ../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f336c1a4a974f04b8404539fbd5be43"}},"metadata":{}},{"name":"stdout","text":"Our text embeddings shape is (40, 768)\nBuilding Model Backbone for tf_efficientnet_b3_ns model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"911ad13ec242426c831057d33f4f1819"}},"metadata":{}},{"name":"stdout","text":"Our image embeddings shape is (40, 1536)\nBuilding Model Backbone for ../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd4a84f1a0b84aab9d64dce76f8df283"}},"metadata":{}},{"name":"stdout","text":"Our text embeddings shape is (40, 768)\nBuilding Model Backbone for tf_efficientnet_b3_ns model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b5ae33b5a0d42828a9df0a749a8cc77"}},"metadata":{}},{"name":"stdout","text":"Our image embeddings shape is (40, 1536)\nBuilding Model Backbone for ../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0255cfbe9e824183b60f48ab58bfa0a2"}},"metadata":{}},{"name":"stdout","text":"Our text embeddings shape is (40, 768)\nBuilding Model Backbone for tf_efficientnet_b3_ns model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1af9c45e9e4f5ca12c38405800a56e"}},"metadata":{}},{"name":"stdout","text":"Our image embeddings shape is (40, 1536)\nBuilding Model Backbone for ../input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer model\n","output_type":"stream"}]},{"cell_type":"code","source":"if CFG.GET_CV:\n#     oof_df['image_predictions'] = image_predictions\n#     oof_df['text_predictions'] = text_predictions\n#     oof_df['pred_matches'] = oof_df.apply(combine_predictions, axis = 1)\n    oof_df['f1'] = f1_score(oof_df['matches'], oof_df['pred_matches'])\n    display(oof_df)\n    score = oof_df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    oof_df.to_csv('oof_df.csv', index=False)\n    oof_df[['posting_id', 'pred_matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions_tfidf\n    df['text_predictions_bert'] = text_predictions_bert\n    df['text_predictions_bert_len'] = df['text_predictions_bert'].apply(lambda x: len(x))\n    df['text_predictions'].mask(df['text_predictions_bert_len'] > 40, df['text_predictions_bert'], inplace=True)\n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('予測個数が2個のものを置き換えるとした時の、BERTのthresholdに対するCVの変化')\n{0.46: 0.8014015026027096,\n 0.45: 0.8014487135564441,\n 0.44: 0.8010120705620158,\n 0.40: 0.8011964254933901,\n 0.35: 0.799005464138435,\n 0.30: 0.7959443282073593}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv').head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}