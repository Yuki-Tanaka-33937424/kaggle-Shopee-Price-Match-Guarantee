{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory settiings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR='./'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "ROOT_DIR = '/home/yuki/shopee/input/shopee-product-matching/'\n",
    "TRAIN_PATH = ROOT_DIR + 'train_images/'\n",
    "TEST_PATH = ROOT_DIR + 'test_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    CHECK_SUB = False\n",
    "    GET_CV = True\n",
    "    num_workers = 4\n",
    "    model_name_cnn = 'tf_efficientnet_b3_ns'\n",
    "    model_name_bert = '/home/yuki/shopee/input/sentence-transformer-models/paraphrase-xlm-r-multilingual-v1/0_Transformer'\n",
    "    size = 512\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    target_size = 8811\n",
    "    target_size_list = [8811, 8812, 8811, 8811, 8811]\n",
    "    target_col = 'label_group'\n",
    "    use_fc = False\n",
    "    use_arcface = True\n",
    "    scale = 30\n",
    "    margin = 0.5\n",
    "    fc_dim = 512\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = False\n",
    "    inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this submission notebook will compute CV score, but commit notebook will not\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/home/yuki/shopee/input/shopee-product-matching/test.csv')\n",
    "if len(test)>3: \n",
    "    CFG.GET_CV = False\n",
    "else: \n",
    "    print('this submission notebook will compute CV score, but commit notebook will not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "sys.path.append('/home/yuki/shopee/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, _LRScheduler\n",
    "\n",
    "import transformers\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n",
    "    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n",
    "    IAAAdditiveGaussianNoise, Transpose\n",
    "    )\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml import PCA\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "import timm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    LOGGER.info(f'[{name}] start')\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'inference.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "#LOGGER = init_logger()\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(CFG.model_name_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if CFG.GET_CV:\n",
    "        \n",
    "        # create folds\n",
    "        # trainingの時と同じようにfoldを切っています。\n",
    "        folds = pd.read_csv('/home/yuki/shopee/input/shopee-product-matching/train.csv')\n",
    "        if CFG.debug:\n",
    "            folds = folds.sample(n=300, random_state=CFG.seed).reset_index(drop=True)  \n",
    "        Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "        groups = folds['label_group'].values\n",
    "        for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_col], groups)):\n",
    "            folds.loc[val_index, 'fold'] = int(n)\n",
    "        folds['fold'] = folds['fold'].astype(int)\n",
    "        display(folds.groupby('fold').size())\n",
    "        \n",
    "        tmp = folds.groupby('label_group')['posting_id'].unique().to_dict()\n",
    "        folds['matches'] = folds['label_group'].map(tmp)\n",
    "        folds['matches'] = folds['matches'].apply(lambda x: ' '.join(x))\n",
    "        folds['file_path'] = folds['image'].apply(lambda x: TRAIN_PATH + x)\n",
    "        \n",
    "        if CFG.CHECK_SUB:\n",
    "            folds = pd.concat([folds, folds], axis=0)\n",
    "            folds.reset_index(drop=True, inplace=True)\n",
    "        folds_cu = cudf.DataFrame(folds)\n",
    "    else:\n",
    "        folds = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        folds['file_path'] = folds['image'].apply(lambda x: TEST_PATH + x)\n",
    "        folds_cu = cudf.DataFrame(folds)\n",
    "        \n",
    "    return folds, folds_cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_paths = df['file_path'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image, torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset_BERT(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['title']\n",
    "        text = tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')  # 'pt': pytorch\n",
    "        input_ids = text['input_ids'][0]\n",
    "        attention_mask = text['attention_mask'][0]\n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            #Resize(CFG.size, CFG.size),\n",
    "            RandomResizedCrop(CFG.size, CFG.size),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(CFG.size, CFG.size),\n",
    "            Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "        \n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "    \n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output, nn.CrossEntropyLoss()(output,label)\n",
    "\n",
    "class CustomEfficientNet(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.target_size, \n",
    "        model_name = CFG.model_name_cnn,\n",
    "        fc_dim = CFG.fc_dim,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = True):\n",
    "        \n",
    "        super(CustomEfficientNet,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        in_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.backbone.global_pool = nn.Identity()\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "        self.use_fc = use_fc\n",
    "        \n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.classifier = nn.Linear(in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            in_features = fc_dim\n",
    "\n",
    "        self.final = ArcMarginProduct(\n",
    "            in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "        \n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "        \n",
    "    def forward(self, image, label):\n",
    "        features = self.extract_features(image)\n",
    "        if self.training:\n",
    "            logits = self.final(features, label)\n",
    "            return logits\n",
    "        else:\n",
    "            return features\n",
    "        \n",
    "    def extract_features(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc and self.training:\n",
    "            x = self.dropout(x)\n",
    "            x = self.classifier(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.target_size,\n",
    "        model_name = CFG.model_name_bert,\n",
    "        fc_dim = CFG.fc_dim,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = CFG.use_fc,\n",
    "        use_arcface = CFG.use_arcface,\n",
    "        pretrained = True):\n",
    "        \n",
    "        super(CustomBERT, self).__init__()\n",
    "        print(f'Building Model Backbone for {model_name} model')\n",
    "        self.bert = transformers.AutoModel.from_pretrained(model_name)\n",
    "        in_features = self.bert.config.hidden_size\n",
    "        self.use_fc = use_fc\n",
    "        self.use_arcface = use_arcface\n",
    "        \n",
    "        if self.use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.classifier = nn.Linear(in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            in_features = fc_dim\n",
    "        \n",
    "        if self.use_arcface:\n",
    "            self.final = ArcMarginProduct(\n",
    "            in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "        else:\n",
    "            self.final = nn.Linear(in_features, n_classes)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        features = self.extract_features(input_ids, attention_mask)\n",
    "        return features\n",
    "        \n",
    "    def extract_features(self, input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        features = x[0]\n",
    "        features = features[:, 0, :]\n",
    "        \n",
    "        if self.use_fc:\n",
    "            features = self.dropout(features)\n",
    "            features = self.classifier(features)\n",
    "            features = self.bn(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(folds, fold):\n",
    "    \n",
    "    CFG.target_size = CFG.target_size_list[fold]\n",
    "    model = CustomEfficientNet(n_classes=CFG.target_size, pretrained=False).to(device)\n",
    "    model_path = f'../input/shopee-002-data-local/tf_efficientnet_b3_ns_fold{fold}_best.pth'\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    image_dataset = TestDataset(folds, transform=get_transforms(data='valid'))\n",
    "    image_loader = DataLoader(image_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n",
    "    embeds = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(image_loader, total=len(image_loader))\n",
    "        for img, label in pbar:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            features = model(img, label)\n",
    "            image_embeddings = features.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "            \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings_infer(folds, fold):\n",
    "    \n",
    "    models = []\n",
    "    for fold in CFG.trn_fold:\n",
    "        CFG.target_size = CFG.target_size_list[fold]\n",
    "        model = CustomEfficientNet(n_classes=CFG.target_size, pretrained=False).to(device)\n",
    "        model_path = f'../input/shopee-002-data-local/tf_efficientnet_b3_ns_fold{fold}_best.pth'\n",
    "        model.load_state_dict(torch.load(model_path)['model'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "    image_dataset = TestDataset(folds, transform=get_transforms(data='valid'))\n",
    "    image_loader = DataLoader(image_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n",
    "    embeds = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(image_loader, total=len(image_loader))\n",
    "        for img, label in pbar:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            features = []\n",
    "            for model in models:\n",
    "                features_ = model(img, label)\n",
    "                features.append(features_.detach().cpu().numpy())\n",
    "            image_embeddings = np.mean(features, axis=0)\n",
    "#             image_embeddings = features\n",
    "            embeds.append(image_embeddings)\n",
    "            \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(folds, fold):\n",
    "    \n",
    "    CFG.target_size = CFG.target_size_list[fold]\n",
    "    model = CustomBERT(n_classes=CFG.target_size, pretrained=False).to(device)\n",
    "    model_path = f'../input/hopee-004-bert-training-data/paraphrase-xlm-r-multilingual-v1_fold{fold}_best.pth'\n",
    "    model.load_state_dict(torch.load(model_path)['model'])\n",
    "    model.eval()\n",
    "    \n",
    "    text_dataset = TestDataset_BERT(folds)\n",
    "    text_loader = DataLoader(text_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              num_workers=CFG.num_workers,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=False)\n",
    "    embeds = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(text_loader, total=len(text_loader))\n",
    "        for input_ids, attention_mask in pbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            features = model(input_ids, attention_mask)\n",
    "            text_embeddings = features.detach().cpu().numpy()\n",
    "            embeds.append(text_embeddings)\n",
    "            \n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_predictions(df, df_cu, max_features=25_000):\n",
    "    \n",
    "    model = TfidfVectorizer(stop_words='english',\n",
    "                            binary=True,\n",
    "                            max_features=max_features)\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    \n",
    "    print('Finding similar titles...')\n",
    "    CHUNK = 1024 * 4\n",
    "    CTS = len(df) // CHUNK\n",
    "    if (len(df)%CHUNK) != 0:\n",
    "        CTS += 1\n",
    "        \n",
    "    preds = []\n",
    "    for j in range( CTS ):\n",
    "        a = j * CHUNK\n",
    "        b = (j+1) * CHUNK\n",
    "        b = min(b, len(df))\n",
    "        print('chunk', a, 'to', b)\n",
    "        \n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "        \n",
    "        for k in range(b-a):\n",
    "            IDX = cupy.where(cts[k,]>0.75)[0]  # 変える余地がありそう\n",
    "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "            preds.append(o)\n",
    "            \n",
    "    del model, text_embeddings\n",
    "    gc.collect()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(df, embeddings, KNN = 50, image = True, thresh_cnn=0.3):\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric='cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "    if CFG.GET_CV:\n",
    "#         if image:\n",
    "#             thresholds = list(np.arange(0.3, 0.5, 0.01))\n",
    "#         else:\n",
    "#             thresholds = list(np.arange(0.4, 0.6, 0.01))  # changed\n",
    "#         scores = []\n",
    "#         for threshold in thresholds:\n",
    "#             predictions = []\n",
    "#             for k in range(embeddings.shape[0]):\n",
    "#                 idx = np.where(distances[k,] < threshold)[0]\n",
    "#                 ids = indices[k, idx]\n",
    "#                 posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "#                 predictions.append(posting_ids)\n",
    "#             df['pred_matches'] = predictions\n",
    "#             df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "#             score = df['f1'].mean()\n",
    "#             print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "#             scores.append(score)\n",
    "#         thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "#         max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "#         best_threshold  = max_score['thresholds'].values[0]\n",
    "#         best_score = max_score['scores'].values[0]\n",
    "#         print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        \n",
    "        # Use threshold\n",
    "        predictions = []\n",
    "        for k in range(embeddings.shape[0]):\n",
    "            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < thresh_cnn)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.3)[0]\n",
    "            ids = indices[k, idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "            \n",
    "    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "    else:\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < thresh_cnn)[0]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 0.3)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    6851\n",
       "1    6849\n",
       "2    6850\n",
       "3    6850\n",
       "4    6850\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>fold</th>\n",
       "      <th>matches</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>3</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>../input/shopee-product-matching/train_images/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>3</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>../input/shopee-product-matching/train_images/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>4</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>../input/shopee-product-matching/train_images/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>3</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>../input/shopee-product-matching/train_images/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>1</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "      <td>../input/shopee-product-matching/train_images/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  fold  \\\n",
       "0                          Paper Bag Victoria Secret    249114794     3   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045     3   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891     4   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188     3   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069     1   \n",
       "\n",
       "                             matches  \\\n",
       "0   train_129225211 train_2278313361   \n",
       "1  train_3386243561 train_3423213080   \n",
       "2  train_2288590299 train_3803689425   \n",
       "3  train_2406599165 train_3342059966   \n",
       "4   train_3369186413 train_921438619   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../input/shopee-product-matching/train_images/...  \n",
       "1  ../input/shopee-product-matching/train_images/...  \n",
       "2  ../input/shopee-product-matching/train_images/...  \n",
       "3  ../input/shopee-product-matching/train_images/...  \n",
       "4  ../input/shopee-product-matching/train_images/...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds, folds_cu = read_dataset()\n",
    "folds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for tf_efficientnet_b3_ns model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d943137c7a74495da0d7d2539997d527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get neighbors for image_embeddings\n",
    "if CFG.GET_CV:\n",
    "    indices = []\n",
    "    image_embeddings = []\n",
    "    text_embeddings = []\n",
    "    for fold in CFG.trn_fold:\n",
    "        folds_ = folds[folds['fold'] == fold]\n",
    "        folds_cu_ = folds_cu[folds['fold'] == fold]\n",
    "        index = folds[folds['fold'] == fold].index\n",
    "        indices.append(list(index))\n",
    "        image_embeddings_ = get_image_embeddings(folds_, fold)\n",
    "        image_embeddings.append(image_embeddings_)\n",
    "        text_embeddings_ = get_text_embeddings(folds_, fold)\n",
    "        text_embeddings.append(text_embeddings_)\n",
    "    # 元のデータの順に並び替える\n",
    "    indices = np.concatenate(indices)\n",
    "    image_embeddings = np.concatenate(image_embeddings)\n",
    "    image_embeddings = image_embeddings[indices]\n",
    "    text_embeddings = np.concatenate(text_embeddings)\n",
    "    text_embeddins = text_embeddins[indices]\n",
    "        \n",
    "    text_predictions_tfidf = get_text_predictions(folds, folds_cu, max_features=25_000)\n",
    "    \n",
    "    for thresh in np.arange(0.2, 0.4, 0.01):\n",
    "        oof_df, image_predictions = get_neighbors(folds, image_embeddings, KNN=50 if len(folds)>3 else 3, image=True, thresh_cnn=thresh)\n",
    "        oof_df, text_predictions_bert = get_neighbors(folds, text_embeddings, KNN=50 if len(folds) > 3 else 3, image=False)\n",
    "        oof_df['image_predictions'] = image_predictions\n",
    "        oof_df['text_predictions'] = text_predictions_tfidf\n",
    "        oof_df['text_predictions_bert'] = text_predictions_bert\n",
    "        oof_df['text_predictions_bert_len'] = oof_df_['text_predictions_bert'].apply(lambda x: len(x))\n",
    "        oof_df['text_predictions'].mask(oof_df_['text_predictions_bert_len'] == 2, oof_df_['text_predictions_bert'], inplace=True)\n",
    "        oof_df['pred_matches'] = oof_df.apply(combine_predictions, axis = 1)\n",
    "        oof_df['f1'] = f1_score(oof_df['matches'], oof_df['pred_matches'])\n",
    "        score = oof_df['f1'].mean()\n",
    "        print(f'Our final f1 cv score for thresh {thresh} is {score}')\n",
    "    oof_df.to_csv('oof_df.csv', index=False)\n",
    "    oof_df[['posting_id', 'pred_matches']].to_csv('submission.csv', index = False)\n",
    "        \n",
    "else:\n",
    "    image_embeddings = get_image_embeddings_infer(folds, fold=0)  # 後で調整する\n",
    "    text_embeddings = get_text_embeddings(folds, fold=0)\n",
    "    text_predictions_tfidf = get_text_predictions(folds, folds_cu, max_features=25_000) \n",
    "    df, text_predictions_bert = get_neighbors(folds, text_embeddings, KNN=50 if len(folds) > 3 else 3, image=False)\n",
    "    df, image_predictions = get_neighbors(folds, image_embeddings, KNN=50 if len(folds)>3 else 3, image=True)\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions_tfidf'] = text_predictions_tfidf\n",
    "    df['text_predictions_bert'] = text_predictions_bert\n",
    "    df['text_predictions_bert_len'] = df['text_predictions_bert'].apply(lambda x: len(x))\n",
    "    df['text_predictions'] = df['text_predictions_tfidf'].mask(df['text_predictions_bert_len'] == 2, df['text_predictions_bert'])\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('submission.csv').head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
